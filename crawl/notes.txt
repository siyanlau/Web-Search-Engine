without file type blacklisting: 
real    0m23.531s
user    0m0.093s
sys     0m0.060s

with file type blacklisting:
real    0m21.236s
user    0m0.046s
sys     0m0.015s

crawling speed can vary a lot without modifying the code in any way?? I was sitting in the same spot, and 20 minutes ago it took 50 seconds to crawl something, now it's 20. Same amount of pages and exact same pages/links

Note on priority scores:
The total_priority values logged for each URL are computed at visit time. Since domain and superdomain counters increase as the crawl progresses, and some URLs redirect to different domains, these logged scores may differ from the priority used when the URL was originally selected from the frontier. This is expected behavior and does not indicate a bug â€” the priority queue always ensures that the crawler pops the highest-scoring URL available at the time of selection.
this phenomenon (priority at processing time is smaller than pre-pop) actually ensures a rough BFS order.

Initial multithreading: robots check and enqueue are the bottlenecks. In the console, you'd see a single worker get stuck on ROBOTS or ENQUEUE tasks, whereas POP and FETCH tasks always moved quickly. 
This is because the original design lets these operations hold down resource locks. The way to resolve it is to make critical sections as small as possible. 
Because our algorithm is biased towards new domains, we actually have to do enqueue and robot checks a lot. So it's important to optimize the implementation for better performance. 


Setting: depth=5, worker=32, page=1000
Original Performance:  time=12m23.947s
After Improvement: time=2m56.622s

I'm still not happy with the robot bottleneck. My crawler spends at least half of its time doing robots compliance. This is when I realized a design issue: I check every URL (100 or less randomly picked links for a single visited page) against robots policy
when in reality this is not necessary, because most of these URLs won't be visited anyways. We can just put all URLs on the PQ, and check on-the-go when a URL is picked. If it violates robots policy, just ignore it.

Original Performance:  time=2m56.622s
After Improvement: time=2m56.622s